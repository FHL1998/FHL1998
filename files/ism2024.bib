@article{FAN20241269,
title = {Unleashing the Potential of Large Language Models for Knowledge Augmentation: A Practical Experiment on Incremental Sheet Forming},
journal = {Procedia Computer Science},
volume = {232},
pages = {1269-1278},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.125},
url = {https://www.sciencedirect.com/science/article/pii/S187705092400125X},
author = {Haolin Fan and Jerry Fuh and Wen Feng Lu and A. Senthil Kumar and Bingbing Li},
keywords = {Incremental Sheet Forming, Large Language Models, Domain Knowledge Augmentation, Fine-tuning},
abstract = {As the influence of Incremental Sheet Forming (ISF) grows in manufacturing sectors, so does the demand for precise and updated knowledge construction in this domain. In this research, we evaluate the capability of Large Language Models (LLMs) to capture domain-specific knowledge, using ISF as a case study. Recognizing common LLMs’ limitations such as potential inaccuracies and outdated information reliance, we propose a comprehensive approach involving automated and adaptive knowledge extraction, enrichment, and integration into an ISF-specific dataset. We then fine-tune the LLMs for ISF-related text classification and prompt response tasks. Our results reveal a significant enhancement in LLMs’ performance within the ISF domain, with a domain knowledge acquisition rate exceeding that of GPT-3.5 by 10.4%, achieved by the fine-tuned Alpaca-33B model. Additionally, we introduce a novel conversational prototype designed to refine the accuracy and relevance of LLMs in the ISF domain. Our findings will guide future efforts in downstream tasks such as ISF-domain knowledge graph construction and quality prediction.}
}